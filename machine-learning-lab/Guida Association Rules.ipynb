{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo8454Hnu4LY"
   },
   "source": [
    "# ASSOCIATION RULES Snippets e Scheletro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nolx0Lj_u4Lb"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variables\n",
    "file_name= 'File_Name.csv'\n",
    "file_name_2 = 'File_Name_2.csv'\n",
    "separator = 'Separator'\n",
    "random_state = 42\n",
    "target = 'Class_Target'\n",
    "\n",
    "# Directives\n",
    "%matplotlib inline\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMyYusowu4Le"
   },
   "outputs": [],
   "source": [
    "# File is a list of list of items\n",
    "# Open the file\n",
    "file = open(file_name, mode = 'r')\n",
    "# The transactions are separated by a ';'\n",
    "# We must also strip the '\\n' at the end of the transaction\n",
    "transactions = [line.strip('\\n').split(separator) for line in file.readlines()]\n",
    "print(f\"First transaction:\\t{transactions[0]}\\n\\nSecond transaction:\\t{transactions[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DII6fWEeu4Le"
   },
   "outputs": [],
   "source": [
    "# Nel caso il file csv ha una colonna index e non è necessaria\n",
    "basket = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ai_XuCmku4Lf"
   },
   "outputs": [],
   "source": [
    "# Nel caso il csv è quadrato (NRow X MColumn) e ci sono celle vuote\n",
    "\n",
    "#Converting the data frame into a list of lists\n",
    "records = []\n",
    "for i in range (0,basket.shape[0]):\n",
    "    records.append([str(basket.values[i,j]) for j in range(0,df.shape[1])])\n",
    "\n",
    "# generate a dataframe basket of boolean values with one row per transaction and one column per distinct item of the database\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "# Encode the transactions\n",
    "encoder = TransactionEncoder()\n",
    "encoded_transactions = encoder.fit_transform(records)\n",
    "# Put the data in a dataframe (boolean value)\n",
    "basket = pd.DataFrame(encoded_transactions.astype(bool), columns = encoder.columns_)\n",
    "# Drop column nan\n",
    "basket = basket.drop(columns = 'nan', axis = 1)\n",
    "basket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVd_INRcu4Lg"
   },
   "outputs": [],
   "source": [
    "# Encode the list of lists into a binary representation and transform it into a dataframe whose columns are the items show the head of that dataframe\n",
    "# Encode something like :\n",
    "# item1;Item2\n",
    "# item3\n",
    "# item4;item1:Item3\n",
    "# in :\n",
    "# TransactionNumber Item1   Item2   Item3   Item4\n",
    "# 0                 1       1       0       0\n",
    "# 1                 0       0       1       0\n",
    "# 2                 1       0       1       1\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "# Encode the transactions\n",
    "encoder = TransactionEncoder()\n",
    "encoded_transactions = encoder.fit_transform(transactions)\n",
    "# Put the data in a dataframe\n",
    "df = pd.DataFrame(encoded_transactions.astype(int), columns = encoder.columns_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uguz5grdO_gz"
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "def get_dataframe(file_name,separator):\n",
    "    # Open the file\n",
    "    file = open(file_name, mode = 'r')\n",
    "    # We must also strip the '\\n' at the end of the transaction\n",
    "    transactions = [line.strip('\\n').split(separator) for line in file.readlines()]\n",
    "    # Encode the transactions\n",
    "    encoder = TransactionEncoder()\n",
    "    encoded_transactions = encoder.fit_transform(transactions)\n",
    "    # Put the data in a dataframe\n",
    "    df = pd.DataFrame(encoded_transactions.astype(int), columns = encoder.columns_)\n",
    "    return transactions,df\n",
    "\n",
    "transactions,df = get_dataframe(file_name,separator)\n",
    "\n",
    "print(f\"First transaction:\\t{transactions[0]}\\n\\nSecond transaction:\\t{transactions[1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk-mR0eRh2FD"
   },
   "outputs": [],
   "source": [
    "# delete row containing less of n_items items\n",
    "n_items = 2\n",
    "single_item_transactions = []\n",
    "# Iterate on all transactions\n",
    "for index, transaction in basket.iterrows():\n",
    "    # If there is just one \"True\" value\n",
    "    count = 0\n",
    "    for item in transaction:\n",
    "        if item == True :\n",
    "            count = count + 1\n",
    "    if count <= n_items:\n",
    "        # Save the index of the transaction\n",
    "        single_item_transactions.append(index)\n",
    "# We can now drop those transactions from the dataframe\n",
    "basket.drop(index = single_item_transactions, axis = 0, inplace = True)\n",
    "single_item_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV4es4Hgu4Lg"
   },
   "outputs": [],
   "source": [
    "# Find a value of min_support such that the apriori algorithm generates at least min_itemsets frequent itemsets\n",
    "#  with\n",
    "# at least min_item_in_itemset items.\n",
    "# Output the result with the message below\n",
    "\n",
    "# Requirements\n",
    "min_itemsets = 8\n",
    "min_item_in_itemset = 2\n",
    "# \"Reasonable\" range\n",
    "support_range = np.arange(0.1, 0.01, -0.01)\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "min_support = 0\n",
    "for s_value in support_range:\n",
    "    print(f\"Trying support value {s_value:.2f}\")\n",
    "    frequent_itemsets = apriori(df, min_support = s_value, use_colnames = True)\n",
    "    # Calculate the number of itemsets that contain at least `min_item_in_itemset` items\n",
    "    # frequent_itemsets must contains al least min_itemsets itemsset , and every item set must contains al least min_item_in_itemset item\n",
    "    itemsets_above_threshold = sum([len(itemset) >= min_item_in_itemset for itemset in frequent_itemsets.itemsets])\n",
    "    if itemsets_above_threshold >= min_itemsets:\n",
    "        min_support = s_value\n",
    "        break\n",
    "if min_support == 0:\n",
    "    print(\"No itemset found! Try again with a bigger range!\")\n",
    "else:\n",
    "    print(f\"I've selected min_support = {min_support:.2f}, which produced␣ , {len(frequent_itemsets)} itemsets, {itemsets_above_threshold} of which had more than {min_item_in_itemset} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R7pG2uKJBB9"
   },
   "outputs": [],
   "source": [
    "# Same as above but with function\n",
    "\n",
    "# Requirements\n",
    "min_itemsets = 8\n",
    "min_item_in_itemset = 2\n",
    "# \"Reasonable\" range\n",
    "support_range = np.arange(0.1, 0.01, -0.01)\n",
    "\n",
    "def get_apriori_info(min_itemsets,min_item_in_itemset,support_range):\n",
    "    min_support = 0\n",
    "    for s_value in support_range:\n",
    "        print(f\"Trying support value {s_value:.2f}\")\n",
    "        frequent_itemsets = apriori(df, min_support = s_value, use_colnames = True)\n",
    "        # Calculate the number of itemsets that contain at least <min_item_in_itemset> items\n",
    "        # frequent_itemsets must contains al least <min_itemsets> itemsset , and every item set must contains al least <min_item_in_itemset> item\n",
    "        itemsets_above_threshold = sum([len(itemset) >= min_item_in_itemset for itemset in frequent_itemsets.itemsets])\n",
    "        if itemsets_above_threshold >= min_itemsets:\n",
    "            min_support = s_value\n",
    "            break\n",
    "\n",
    "    return min_support,itemsets_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzlLw9Elu4Lh"
   },
   "outputs": [],
   "source": [
    "# Find the minimum metric threshold such that at least min_association_rule association rules are extracted from the frequent itemsets found\n",
    "\n",
    "#Requirment\n",
    "metric_threshold_range = np.arange(20, 0.01, -0.01)\n",
    "min_association_rule = 10\n",
    "min_metric_threshold = 0\n",
    "association_rule_found = 0\n",
    "current_metric = \"lift\"\n",
    "\n",
    "for metric_value in metric_threshold_range:\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric=current_metric, min_threshold=metric_value)\n",
    "    if rules.shape[0] >= min_association_rule:\n",
    "        association_rule_found = rules.shape[0]\n",
    "        min_metric_threshold = metric_value\n",
    "        break\n",
    "\n",
    "if association_rule_found == 0:\n",
    "    print(\"No association rule! Try again with a bigger range!\")\n",
    "else:\n",
    "    print(f\"I've selected metric {current_metric}  with metric_value = {metric_value:.2f}, which produced , {association_rule_found} association_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFF6PiWOOLPh"
   },
   "outputs": [],
   "source": [
    "# Find the minimum metric threshold such that at least <min_association_rule> association rules are extracted from the frequent itemsets found\n",
    "def get_metric_info(metric_threshold_range,frequent_itemsets,metric,min_association_rule):\n",
    "    min_metric_threshold = 0\n",
    "    association_rule_found = 0\n",
    "    for metric_value in metric_threshold_range:\n",
    "        rules = association_rules(frequent_itemsets, metric=metric, min_threshold=metric_value)\n",
    "        if rules.shape[0] >= min_association_rule:\n",
    "            association_rule_found = rules.shape[0]\n",
    "            min_metric_threshold = metric_value\n",
    "            break\n",
    "    return min_metric_threshold,association_rule_found\n",
    "\n",
    "metric_threshold_range = np.arange(20, 0.01, -0.01)\n",
    "min_association_rule = 10\n",
    "min_metric_threshold,association_rule_found = get_metric_info(metric_threshold_range,frequent_itemsets,'lift',min_association_rule)\n",
    "if association_rule_found == 0:\n",
    "    print(\"No association rule! Try again with a bigger range!\")\n",
    "else:\n",
    "    print(f\"I've selected metric_value = {metric_value:.2f}, which produced , {association_rule_found} association_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4Te7T3xu4Li"
   },
   "outputs": [],
   "source": [
    "# Print the first n rules found, sorted by descending confidence and support\n",
    "n_rule = 10\n",
    "sorted_rules=rules.sort_values(by=['confidence','support'],ascending=False).reset_index(drop=True)\n",
    "sorted_rules.head(n_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL0xn9v1u4Li"
   },
   "outputs": [],
   "source": [
    "# Plot confidence and support for all the sorted rules found\n",
    "sorted_rules[['confidence','support']].plot(title='Association Rules');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwaDpUJvu4Lj"
   },
   "outputs": [],
   "source": [
    "# Scatter plot the rules by confidence and support, labelling the points with the index value of the corresponding rule\n",
    "# size_point is chosen empirically to obtain the best graphical effect\n",
    "size_point = 1.4\n",
    "# The size of each point\n",
    "s = [size_point**n for n in rules.lift]\n",
    "\n",
    "rules.plot.scatter(x='support',\n",
    "                   y='confidence',\n",
    "                   title='Association Rules (dot proportional to Lift)',\n",
    "                   s=s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNbsW-5pu4Lj"
   },
   "outputs": [],
   "source": [
    "# Scatter plot the rules by confidence and support, labelling the points with the index value of the corresponding rule\n",
    "fig = sorted_rules.plot.scatter(\n",
    "                                x='confidence',\n",
    "                                y='support',\n",
    "                                title='Association Rules'\n",
    "                                )\n",
    "\n",
    "# Iterate over all the rules and annotate them with their index\n",
    "for i in range(len(sorted_rules)):\n",
    "    fig.annotate(text = i, xy = (sorted_rules['confidence'][i], sorted_rules['support'][i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
