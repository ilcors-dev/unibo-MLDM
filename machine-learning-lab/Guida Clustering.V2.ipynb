{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1NmkRjTuzon"
   },
   "source": [
    "# CLUSTERING Snippets e Scheletro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC9W0c_4uzos"
   },
   "source": [
    "## 1. Elaborazione dei Dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_xj6UPHuzot"
   },
   "source": [
    "### Import e preparazione delle strutture dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_9DpSNPuzou"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variables\n",
    "file_name= 'File_Name.csv'\n",
    "file_name_2 = 'File_Name_2.csv'\n",
    "separator = 'Separator'\n",
    "random_state = 42\n",
    "target = 'Class_Target'\n",
    "\n",
    "# Directives\n",
    "%matplotlib inline\n",
    "np.random.seed(random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEmebX1fuzox"
   },
   "source": [
    "### Caricamento delle strutture dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy_3J4HLuzoy"
   },
   "outputs": [],
   "source": [
    "# Load file (Prima riga ci sono le label e la prima colonna ha gli indici)\n",
    "df = pd.read_csv(file_name, delimiter = separator, header = 0, index_col = 0)\n",
    "\n",
    "# Load file (DataSet senza label e indici)\n",
    "df = pd.read_csv(file_name, delimiter = separator, header=None, index_col=None)\n",
    "\n",
    "# Load file (DataSet con names)\n",
    "df = pd.read_csv(file_name, delimiter = separator, header=None, index_col=None, names=['colonna1', 'colonna2'])\n",
    "\n",
    "# Load data from a .txt file\n",
    "text = np.loadtxt(file_name, delimiter = separator)\n",
    "df = pd.DataFrame(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCEr4vOuuzoz"
   },
   "source": [
    "### Mostra dei dati (SIZE, DESCRIBE, BOXPLOT, PAIRPLOT, CORRELATION MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKx6k36ruzoz"
   },
   "outputs": [],
   "source": [
    "# Show the DataFrame (All)\n",
    "df\n",
    "\n",
    "# Show Structure\n",
    "df.describe()\n",
    "\n",
    "# Show the head of the dataframe\n",
    "df.head()\n",
    "\n",
    "# For each column show the frequencies of each distinct value\n",
    "np.unique(df, return_counts = True)\n",
    "\n",
    "# Show the number of rows and columns\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns in this dataset\")\n",
    "\n",
    "# Show Shape\n",
    "print (\"The shape is: {}\".format(df.shape))\n",
    "\n",
    "# Show the size of the dataframe\n",
    "print(f\"The dataframe has size: {df.size}\")\n",
    "\n",
    "# Pairplot (relazioni fra attributi rispetto al target)\n",
    "# NON TIENE VALORI STRINGHE (NO ERRORI)\n",
    "sns.pairplot(df, hue = target)\n",
    "\n",
    "#come selezionare colonne : selezionata colonna 1 e 2\n",
    "pippo = X[[1,2]]\n",
    "pippo\n",
    "sns.pairplot(pippo)\n",
    "\n",
    "\n",
    "\n",
    "# Boxplot (trovare Outliers)\n",
    "# NON TIENE VALORI STRINGHE (DA ERRORI, DA TOGLIERE)\n",
    "plt.figure(figsize=(15,15))\n",
    "pos = 1\n",
    "for i in df.columns:\n",
    "        if(type(df[i][0]) != str):\n",
    "                plt.subplot(4, 3, pos)\n",
    "                sns.boxplot(df[i])\n",
    "                pos += 1\n",
    "\n",
    "# Correlation Matrix\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "#Check the number of rows with missing values\n",
    "rows_missingvalues = df.isna().any(axis=1).sum()\n",
    "print(\"Rows with missing values: {}\".format(rows_missingvalues))\n",
    "\n",
    "# Histogram of numeric data\n",
    "pd.DataFrame.hist(df, figsize=[15,15]);\n",
    "\n",
    "# Histogram of the column target (even if a string)\n",
    "df['target'].hist()\n",
    "\n",
    "# Scatter Plot (X column 0 and Y column 1 of df)\n",
    "sns.scatterplot(x=focus[0], y=focus[1], data=df, hue=\"target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRSPCQP6uzo0"
   },
   "source": [
    "### Esempi di Commenti sui grafici dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0v_XWahTuzo1"
   },
   "outputs": [],
   "source": [
    "# The boxplots show that there are no outliers, the distribution of 0 and 3 is very similar.\n",
    "#  1 and 2 have a similar median value but different distribution of values. There doesn't seem to be any particular situation showing.\n",
    "\n",
    "# From the pairplot it is clear that the columns 1 and 2 tend to form quite distinct clusters. They're probably our best bet for our clustering efforts.\n",
    "\n",
    "# Column 1 and 2 are the most interesting attribute. !!! 1 e 2 sono gli attributi piu' facilmente separabili\n",
    "\n",
    "# Both the silhouette scores and the inertia elbow suggest that the best number of clusters is 4,  !!! Quando si stampa silhouette e inertia\n",
    "# which is in line with what we were expecting, given the initial pairplots\n",
    "\n",
    "# The pairplots don't seem to show any particular pattern in the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3YYFq7luzo2"
   },
   "source": [
    "### Modifica del DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wglDc2euzo2"
   },
   "outputs": [],
   "source": [
    "# Merge the two dataframes with the 'outer' how, as to perform a SQL-like full outer join\n",
    "# on the two indexes, adding suffixes as requested (default option)\n",
    "# (Entrambi hanno Indici e prima riga Label da differenziare Target)\n",
    "df = first_df.merge(second_df, how = 'outer', left_index = True, right_index = True, suffixes = ('_x', '_y'))\n",
    "\n",
    "# Drop those rows from the dataframe\n",
    "df = df.drop(index = indexes_to_delete, axis = 0)\n",
    "\n",
    "# Drop specific column\n",
    "df = df.drop(columns = 'Column_Name', axis = 1)\n",
    "\n",
    "# Drop more than 1 column\n",
    "df = df.drop(columns = ['Column_Name1', 'Column_Name2'], axis = 1)\n",
    "\n",
    "# Rename specific column\n",
    "df = df.rename(columns = {'Old_Name1':'New_Name1', 'Old_Name2':'New_name2'})\n",
    "\n",
    "# Get the column names\n",
    "column_names = list(df.columns)\n",
    "\n",
    "# Reindex the dataframe\n",
    "df = df.reindex(columns = column_names)\n",
    "\n",
    "# Eliminate the rows containing null values\n",
    "df = df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YfWskv6uzo3"
   },
   "outputs": [],
   "source": [
    "# assegnare dei nomi alle colonne se in dataset originale non ha nomi alle colonne\n",
    "columns =[]\n",
    "for i in range(df.shape[1]):\n",
    "    columns.append(i)\n",
    "\n",
    "df.columns = columns\n",
    "\n",
    "# assegnare dei nomi alle colonne se in dataset originale non ha nomi alle colonne\n",
    "columns =[]\n",
    "for i in range(df.shape[1]):\n",
    "    columns.append(i)\n",
    "\n",
    "# last element\n",
    "columns[-1] = 'Class_target'\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKlG00iauzo3"
   },
   "source": [
    "### Trasformazione dei dati per Grafici o altro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUHopqZ2uzo6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "column_target = 'target'\n",
    "\n",
    "# Set the transformer data type (if required)\n",
    "transf_dtype = np.int32\n",
    "\n",
    "# OneHotEncoder (da Nominal a Numerical)\n",
    "encoder = OneHotEncoder(dtype = transf_dtype)\n",
    "transformed = encoder.fit_transform(df[[column_target]])\n",
    "df[encoder.categories_[0]] = transformed.toarray()\n",
    "df = df.drop(column_target, axis = 1)\n",
    "\n",
    "# OrdinalEncoder (da Ordinal a Numerical)\n",
    "encoder = OrdinalEncoder(dtype = transf_dtype)\n",
    "df[column_target] = encoder.fit_transform(df[[column_target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXk6xvB-uzo6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# We will transform into integers\n",
    "transf_dtype = np.int32\n",
    "encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False, dtype = transf_dtype)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_e = encoder.fit_transform(df)\n",
    "X_ohe = pd.DataFrame(X_e)\n",
    "X_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xBaADadW_tE"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_ohe(df):\n",
    "  # We will transform into integers\n",
    "  transf_dtype = np.int32\n",
    "  encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False, dtype = transf_dtype)\n",
    "\n",
    "  # Fit and transform the data\n",
    "  X_e = encoder.fit_transform(df)\n",
    "  X_ohe = pd.DataFrame(X_e)\n",
    "  return X_ohe\n",
    "\n",
    "X_ohe = get_ohe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqbQR2oNn6Xf"
   },
   "outputs": [],
   "source": [
    "# Sex ha valori : M,F,I =>\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "df['Sex'] = oe.fit_transform(df['Sex'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGj_WNosuzo7"
   },
   "source": [
    "### Snippets utili (Liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXaQXmX9uzo7"
   },
   "outputs": [],
   "source": [
    "# Remove an item (target) from a list\n",
    "list_name.remove(target)\n",
    "\n",
    "# Sort the values\n",
    "list_name.sort()\n",
    "\n",
    "# Append an item (target) to a list (put it last)\n",
    "list_name.append(target)\n",
    "\n",
    "\n",
    "# Sorting tuple\n",
    "# For example : A sorted list of the discovered clusters for decreasing sizes\n",
    "# df : dataframe\n",
    "# \"cluster_n\" : column name of the clusters\n",
    "val, counts = np.unique(df[\"cluster_n\"], return_counts=True)\n",
    "val_count = []\n",
    "for i in range(0,len(val)):\n",
    "    val_count.append([counts[i], val[i]])\n",
    "# for decreasing sizes => reverse=True\n",
    "val_count.sort(key=lambda x: x[0],reverse=True)\n",
    "print(val_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBKGROSOuzo7"
   },
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WD0-klknuzo7"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLHJ-5V4uzo8"
   },
   "source": [
    "## 2. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maRYIbJpuzo8"
   },
   "source": [
    "* Il cluster si applica a valori numerici X, quindi vanno tolte (o trasformate) quelle colonne di valori stringhe\n",
    "* Il salvataggio degli altri parametri potrebbe essere messo in vettore y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGZta3hjuzo8"
   },
   "outputs": [],
   "source": [
    "target_column = 'y'\n",
    "\n",
    "# Separate in X all the columns but the last one\n",
    "X = df.drop(target_column, axis = 1)\n",
    "\n",
    "# Save the last column in y\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nD9ZGQXduzo8"
   },
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxIjEq4Puzo8"
   },
   "source": [
    "#### i. Trovare parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dbQXNdhuzo9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Range of possible clusters\n",
    "k_range = range(2,11)\n",
    "\n",
    "# Distortion, Silhouette Score and size deviation as measures\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "size_deviation = []\n",
    "\n",
    "for i in k_range:\n",
    "\n",
    "    # Iterate over our range of possible clusters\n",
    "    km = KMeans(n_clusters = i,\n",
    "                init = 'k-means++',\n",
    "                n_init = 10,\n",
    "                max_iter = 300,\n",
    "                random_state = random_state)\n",
    "\n",
    "    # Fit predict\n",
    "    y_km = km.fit_predict(X)\n",
    "\n",
    "    # Compute the deviation with the provided formula\n",
    "    deviation = np.sqrt(np.unique(y_km, return_counts = True)[1].var())/i\n",
    "\n",
    "    # Store the data in the arrays\n",
    "    distortions.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X,y_km))\n",
    "    size_deviation.append(deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6lC56uouzo9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "def km_parameters(min_n_cluster,max_n_cluster,random_state,X):\n",
    "    # Range of possible clusters\n",
    "    k_range = range(min_n_cluster,max_n_cluster)\n",
    "\n",
    "    # Distortion, Silhouette Score and size deviation as measures\n",
    "    distortions = []\n",
    "    silhouette_scores = []\n",
    "    size_deviation = []\n",
    "\n",
    "    for i in k_range:\n",
    "\n",
    "        # Iterate over our range of possible clusters\n",
    "        km = KMeans(n_clusters = i,\n",
    "                    init = 'k-means++',\n",
    "                    n_init = 10,\n",
    "                    max_iter = 300,\n",
    "                    random_state = random_state)\n",
    "\n",
    "        # Fit predict\n",
    "        y_km = km.fit_predict(X)\n",
    "\n",
    "        # Compute the deviation with the provided formula\n",
    "        deviation = np.sqrt(np.unique(y_km, return_counts = True)[1].var())/i\n",
    "\n",
    "        # Store the data in the arrays\n",
    "        distortions.append(km.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X,y_km))\n",
    "        size_deviation.append(deviation)\n",
    "\n",
    "    return distortions,silhouette_scores,size_deviation,k_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbdWkZKfzqAJ"
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=k,\n",
    "            random_state=random_state)\n",
    "y_km = km.fit_predict(X)\n",
    "print(\"Number of clusters = {}\\t- Distortion = {:6.2f}\\t- Silhouette score = {:4.2f}\"\\\n",
    "    .format(k,distortions[k_range.index(k)],silhouette_scores[k_range.index(k)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83Wb2_Eh2f9b"
   },
   "outputs": [],
   "source": [
    "# Size of a cluster\n",
    "clust_sizes_km = np.unique(y_km,return_counts=True)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "data = clust_sizes_km[1]\n",
    "labels = clust_sizes_km[0]\n",
    "plt.pie(data,\n",
    "    labels = labels,\n",
    "    autopct='%1.1f%%',\n",
    "    shadow=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print clust_sizes_km in oder to have size of the cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv-lpxevuzo9"
   },
   "source": [
    "#### ii. Grafico Inertia - Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8XRaQUuuzo9"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of clusters')\n",
    "ax1.set_ylabel('Inertia', color=color)\n",
    "ax1.plot(k_range, distortions, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Silhouette scores', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(k_range, silhouette_scores, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0,1) # the axis for silhouette is [0,1]\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQwqiSuZuzo-"
   },
   "source": [
    "#### ii.a Grafico generico 2 parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_xVIQP1uzo-"
   },
   "outputs": [],
   "source": [
    "def two_plots(x, y1, y2, xlabel, y1label, y2label):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(y1label, color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2label, color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_ylim(0,1) # the axis for silhouette is [0,1]\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "# inertias_km e silhouette_scores_km potrebbero essere presi da  i. Trovare parametri\n",
    "two_plots(x=k_range, y1=inertias_km, y2=silhouette_scores_km, xlabel='Number of clusters', y1label='Inertias', y2label='Silhouette scores' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kPvWubIuzo-"
   },
   "source": [
    "#### iii. Grafico Inertia - Size deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ol-xTQTtuzo-"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of clusters')\n",
    "ax1.set_ylabel('Inertia', color=color)\n",
    "ax1.plot(k_range, distortions, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Size deviation index', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(k_range, size_deviation, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78-jxistuzo_"
   },
   "source": [
    "#### iv. Grafico Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sY60tWy5uzo_"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Silhouette scores', color=color)\n",
    "ax.plot(k_range, silhouette_scores, color=color)\n",
    "ax.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54LuNinLuzo_"
   },
   "source": [
    "#### v. Grafico Silhouette con plot_silhouette. A plot of the silhouette index for the data points, grouped according to the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nXISnW5uzo_"
   },
   "outputs": [],
   "source": [
    "from plot_silhouette import plot_silhouette\n",
    "\n",
    "# IF IT DOESN'T WORK TRY TRANSFORMING X TO AN ARRAY\n",
    "# X_array = X.to_numpy()\n",
    "\n",
    "# produce the silhouette plot using the function plot_silhouette\n",
    "silhouette_score_samples = silhouette_samples(X, y_km, metric='euclidean')\n",
    "plt.title(f'Silhouette score for samples with {best_k} clusters')\n",
    "plot_silhouette(silhouette_score_samples, y_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4zk0Z5_598y"
   },
   "source": [
    "V.1 Grafico Silhouette con plot_silhouette e silhouette sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8a5BC9358Md"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "silhouette = silhouette_samples(X,y_km)\n",
    "# from plot_silhouette import plot_silhouette  # python script provided separately\n",
    "plot_silhouette(silhouette,y_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3XxxfNauzpA"
   },
   "source": [
    "#### vi. Istanziamento + fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD57zzcXuzpA"
   },
   "outputs": [],
   "source": [
    "## CAMBIARE PARAMETRO CLUSTER\n",
    "# Re-instantiate\n",
    "km = KMeans(n_clusters = 3, init = 'k-means++', n_init = 10, max_iter = 300, random_state = random_state)\n",
    "\n",
    "# Fit and predict\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1GqjXf6uzpB"
   },
   "outputs": [],
   "source": [
    "print(\"Number of clusters = {}\\t- Distortion = {:6.2f}\\t- Silhouette score = {:4.2f}\".format(k,distortions[k_range.index(k)],silhouette_scores[k_range.index(k)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2ErGWIIuzpC"
   },
   "source": [
    "#### vii. plot pairplot . Input : dataframe , numero_di_cluster , random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QFki3q9uzpC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_pairplot(df,n_cluster,random_state):\n",
    "    best_km = KMeans(n_cluster, init = 'k-means++', random_state=random_state)\n",
    "    y_best_km = best_km.fit_predict(df)\n",
    "    target = \"cluster_n\"\n",
    "    df_y = df\n",
    "    df_y[target] = y_best_km\n",
    "    sns.pairplot(df_y, hue = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1xdPSVUuzpC"
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XviTHpDyuzpC"
   },
   "source": [
    "#### i. Trovare parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9etK7MWFuzpC"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# !!! ATTENZIONE alla distanza tra i punti !!!\n",
    "# param_grid = {'eps': list(np.arange(0.01, 1, 0.01)), 'min_samples': list(range(1,10,1))}\n",
    "# param_grid = {'eps': list(np.arange(60, 120, 20)), 'min_samples': list(range(5,30,5))}\n",
    "params = list(ParameterGrid(param_grid))\n",
    "\n",
    "dbscan_out = pd.DataFrame(columns = ['eps','min_samples','n_clusters', 'size deviation index','silhouette', 'unclust%'])        #0 righe, 5 colonne\n",
    "\n",
    "for i in range(len(params)):\n",
    "    db = DBSCAN(**(params[i]))\n",
    "    y_db = db.fit_predict(X)\n",
    "                                                                            #prende valori e filtra rumore\n",
    "    cluster_labels_all = np.unique(y_db)\n",
    "    cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "    n_clusters = len(cluster_labels)\n",
    "\n",
    "    if n_clusters > 1:\n",
    "        X_cl = X.iloc[y_db!=-1,:]                                                #filtra il rumore da X\n",
    "        y_db_cl = y_db[y_db!=-1]                                            #filtra il rumore da Y\n",
    "\n",
    "        silhouette = silhouette_score(X_cl,y_db_cl)\n",
    "        deviation = np.sqrt(np.unique(y_db_cl, return_counts = True)[1].var())/i\n",
    "        uncl_p = (1 - y_db_cl.shape[0]/y_db.shape[0]) * 100                 #% di dati unclustered\n",
    "        dbscan_out.loc[len(dbscan_out)] = [db.eps, db.min_samples, n_clusters, deviation, silhouette, uncl_p]\n",
    "        print(\"{:11.2f}\\t{:11}\\t{:11}\\t{:11.2f}\\t{:11.2f}\\t{:11.2f}\".format(db.eps, db.min_samples, n_clusters, silhouette, uncl_p, s_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUp3T9O8uzpD"
   },
   "outputs": [],
   "source": [
    "sil_thr = 0.7  # visualize results only for combinations with silhouette above the threshold\n",
    "unc_thr = 100 # visualize results only for combinations with unclustered% below the threshold\n",
    "n_clu_max_thr = 10\n",
    "\n",
    "                                                                        #Filtro\n",
    "db_sort = dbscan_out[(dbscan_out['silhouette']>=sil_thr)\\\n",
    "         & (dbscan_out['unclust%']<=unc_thr)\\\n",
    "         & (dbscan_out['n_clusters']<=n_clu_max_thr)]\n",
    "\n",
    "db_sort.sort_values('silhouette', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGuA73DxuzpD"
   },
   "outputs": [],
   "source": [
    "# Metodo alternativo per trovare i parametri con stampa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3PS4tFEuzpD"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# !!!! ATTENZIONE  !!!!\n",
    "# Versione di param_grid con variabili continue\n",
    "param_grid = {'eps': list(np.arange(0.01, 1, 0.01)), 'min_samples': list(range(1,10,1))}\n",
    "# Versione di param_grid con veriabili discrete tipo OneHotEncoding\n",
    "# Valori di esempio\n",
    "number_of_column = 30 # df.shape[1]\n",
    "number_of_row = 1000 # df.shape[0]\n",
    "delta_item = 10\n",
    "min_number_of_item_for_cluster = 30\n",
    "\n",
    "param_grid = {'eps': list(np.arange(1, number_of_column, 1)),\n",
    "              'min_samples': list(range(min_number_of_item_for_cluster,number_of_row,delta_item))}\n",
    "params = list(ParameterGrid(param_grid))\n",
    "\n",
    "dbscan_out = pd.DataFrame(columns = ['eps','min_samples','n_clusters', 'size deviation index','silhouette', 'unclust%'])        #0 righe, 5 colonne\n",
    "print(\"{:11}\\t{:11}\\t{:11}\\t{:11}\\t{:11}\\t{:11}\".format('        eps','min_samples',' n_clusters',' silhouette', '    unclust%', '    size deviation'))\n",
    "\n",
    "for i in range(len(params)):\n",
    "    db = DBSCAN(**(params[i]))\n",
    "    y_db = db.fit_predict(df_ohe)\n",
    "\n",
    "    cluster_labels_all = np.unique(y_db)\n",
    "    cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "    n_clusters = len(cluster_labels)\n",
    "\n",
    "    if n_clusters > 1:\n",
    "        X_cl = df_ohe.iloc[y_db!=-1,:]\n",
    "        y_db_cl = y_db[y_db!=-1]\n",
    "\n",
    "        silhouette = silhouette_score(X_cl,y_db_cl)\n",
    "        deviation = np.sqrt(np.unique(y_db_cl, return_counts = True)[1].var())/i\n",
    "        uncl_p = (1 - y_db_cl.shape[0]/y_db.shape[0]) * 100\n",
    "        dbscan_out.loc[len(dbscan_out)] = [db.eps, db.min_samples, n_clusters, deviation, silhouette, uncl_p]\n",
    "        print(\"{:11.2f}\\t{:11}\\t{:11}\\t{:11.2f}\\t{:11.2f}\\t{:11.2f}\".format(db.eps, db.min_samples, n_clusters, silhouette, uncl_p, deviation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5o2HfSc2I0F"
   },
   "source": [
    "Size of a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOk1uvrV2N9K"
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps = 100.00, min_samples = 15)\n",
    "\n",
    "# Fit and predict\n",
    "y_db = db.fit_predict(X.to_numpy())\n",
    "y_db_cl = y_db[y_db!=-1]                                            #filtra il rumore da Y\n",
    "\n",
    "clust_sizes_km = np.unique(y_db_cl, return_counts=True)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "data = clust_sizes_km[1]\n",
    "labels = clust_sizes_km[0]\n",
    "plt.pie(data,\n",
    "    labels = labels,\n",
    "    autopct='%1.1f%%',\n",
    "    shadow=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo6uqDhkuzpE"
   },
   "source": [
    "#### ii. Istanziamento + fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE-bIVUZuzpE"
   },
   "outputs": [],
   "source": [
    "## CAMBIARE PARAMETRI\n",
    "# Re-instantiate\n",
    "db = DBSCAN(eps = 0.14, min_samples = 7)\n",
    "\n",
    "# Fit and predict\n",
    "y_db = db.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LtxB8VwuzpE"
   },
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glMi1BAbuzpF"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "parameters = [{'n_clusters': k_range\n",
    "                    , 'linkage' : ['ward', 'complete', 'average', 'single']}]\n",
    "pg = list(ParameterGrid(parameters))\n",
    "result_ac = []\n",
    "for i in range(len(pg)):\n",
    "    ac = AgglomerativeClustering(**(pg[i]))\n",
    "    y_ac = ac.fit_predict(X)\n",
    "    deviation = np.sqrt(np.unique(y_ac, return_counts = True)[1].var())/i\n",
    "    result_ac.append([pg[i]['linkage'],pg[i]['n_clusters'], silhouette_score(X,y_ac), deviation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SOFQ6MquzpF"
   },
   "outputs": [],
   "source": [
    "df_result_ac = pd.DataFrame(data = result_ac, columns=['linkage','n_clusters','silhouette_score', 'deviation'])\n",
    "df_result_ac.sort_values(by='silhouette_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86AhU6pjuzpF"
   },
   "source": [
    "#### Istanziamento + fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD3csUvWuzpG"
   },
   "outputs": [],
   "source": [
    "## CAMBIARE PARAMETRO CLUSTER\n",
    "# Re-instantiate\n",
    "ac = AgglomerativeClustering(n_clusters=5)\n",
    "\n",
    "# Fit and predict\n",
    "y_ac = ac.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVoD65VyuzpG"
   },
   "source": [
    "## 3. Confronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCXMdqS7uzpG"
   },
   "source": [
    "### Performance con migliori parametri, matrici di confusione (Pair) e adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-wpLNHDuzpG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Output the silhouette score\n",
    "print(f\"The silhouette score for best_k = 3 was {silhouette_score(X,y_km)}\")\n",
    "\n",
    "# Pairplot con classe generata dal cluster\n",
    "sns.pairplot(X, hue = 'Target')\n",
    "\n",
    "# Adjusted_rand_score\n",
    "adjusted_rand_score(y_km, y_db)\n",
    "\n",
    "# Pair_confusion_matrix (Discrimina come TP FP TN FN)\n",
    "pair_confusion_matrix(y_km, y_db)\n",
    "\n",
    "#Confusion matrix (Confronta valori della y)\n",
    "cm = confusion_matrix(y, y_ac)\n",
    "CMD = ConfusionMatrixDisplay(cm)\n",
    "CMD.plot()\n",
    "\n",
    "# Accuracy scores\n",
    "DB_accuracy = accuracy_score(y, y_db) * 100\n",
    "print(f\"The accuracy for DB was {DB_accuracy:.2f}%\")\n",
    "\n",
    "KM_accuracy = accuracy_score(y, y_km) * 100\n",
    "print(f\"The accuracy for KM was {KM_accuracy:.2f}%\")\n",
    "\n",
    "AC_accuracy = accuracy_score(y, y_ac) * 100\n",
    "print(f\"The accuracy for AC was {AC_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxL736AKuzpH"
   },
   "source": [
    "### A sorted list of the discovered clusters for decreasing sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lRI7u0juzpH"
   },
   "outputs": [],
   "source": [
    "val, counts = np.unique(df[\"cluster\"], return_counts=True)\n",
    "val_count = []\n",
    "for i in range(0,len(val)):\n",
    "    val_count.append([counts[i], val[i]])\n",
    "val_count.sort(reverse=True)\n",
    "sorted_clusters = [i[1] for i in val_count]\n",
    "sorted_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku1SlI6QuzpH"
   },
   "source": [
    "## 4. Gold Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mGdrpLDuzpI"
   },
   "source": [
    "### Rimappare gli y_km derivati dal clustering secondo il gold standard y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-4Qm_sGuzpI"
   },
   "outputs": [],
   "source": [
    "# For this algorithm to work correctly we need to have\n",
    "# a match in the first iteration\n",
    "previous_label = y_km[0]\n",
    "start_idx = 0\n",
    "changes = 0\n",
    "\n",
    "for idx, label in enumerate(y_km):\n",
    "\n",
    "    # Apply remapping\n",
    "    if label != previous_label:\n",
    "\n",
    "        # Count the occurrences in the corresponding\n",
    "        # y_km subset\n",
    "        occurrences = np.bincount(y[start_idx:idx])\n",
    "\n",
    "        # Find which cluster index is the most frequent\n",
    "        gold_standard = np.argmax(occurrences)\n",
    "\n",
    "        # Remap the clusters\n",
    "        for i in range(start_idx, idx):\n",
    "\n",
    "            if y_km[i] != gold_standard:\n",
    "                y_km[i] = gold_standard\n",
    "                changes = changes + 1\n",
    "\n",
    "        # The current index will now be the new start idx\n",
    "        start_idx = idx\n",
    "\n",
    "    # Save current label as 'previous_label'\n",
    "    previous_label = label\n",
    "\n",
    "# Output the silhouette score\n",
    "print(f\"This remapping performed {changes} changes to y_km subsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVktC40AuzpI"
   },
   "source": [
    "## 5. Trasformazioni dei valori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgMX6gdMuzpI"
   },
   "source": [
    "### Miglioramento dei risultati (operazione Logaritmica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZQ-kSBZuzpJ"
   },
   "outputs": [],
   "source": [
    "# Iterate over column names\n",
    "for column in X:\n",
    "\n",
    "    # Select column contents by column\n",
    "    # name using [] operator\n",
    "    if( not (X[column] <= 0).any() ):\n",
    "        X[column] = np.log(X[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfXogDkNuzpJ"
   },
   "source": [
    "### MinMax Scaler: trasformazione tutti valori in un range da 0 a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_B5W0PwuzpJ"
   },
   "outputs": [],
   "source": [
    "# remap on the 0:1 range with MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X = pd.DataFrame(mms.fit_transform(X), columns = X.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Q09rBkuzpJ"
   },
   "source": [
    "### Square Root transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vqeu3WP3uzpK"
   },
   "outputs": [],
   "source": [
    "# square root transformation - the first two columns are not transformed\n",
    "from math import sqrt\n",
    "\n",
    "X_sqrt = pd.concat([X.iloc[:,:2],X.iloc[:,2:].applymap(sqrt)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3naXVXxcuzpK"
   },
   "source": [
    "### MaxDiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YM7Oat7XuzpK"
   },
   "outputs": [],
   "source": [
    "# Professor function\n",
    "from max_diag import max_diag\n",
    "\n",
    "# Apply on a confusion matrix\n",
    "cm_km = max_diag(cm)\n",
    "CMD = ConfusionMatrixDisplay(cm_km)\n",
    "CMD.plot()\n",
    "\n",
    "# To see why it is useful\n",
    "help(max_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvjWLk8d4g_t"
   },
   "source": [
    "### PowerTransformer,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yhWJCB-4nDl"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "preprocessor = make_pipeline(StandardScaler(with_std=False),\n",
    "                        PowerTransformer(standardize=True))\n",
    "X_fit= X.copy()\n",
    "X_fit= preprocessor.fit_transform(X_fit)\n",
    "X_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc0PYz-UuzpK"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "1873f118b511de53363c30a8f94ef950fb879bc21455477d7c0f6a051a0155e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
